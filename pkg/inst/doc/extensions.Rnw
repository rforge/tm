\documentclass[a4paper]{article}

\usepackage[margin=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
\usepackage{url}

\newcommand{\acronym}[1]{\textsc{#1}}
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\let\code\texttt

%% \VignetteIndexEntry{Extensions}

\begin{document}
<<Init,echo=FALSE,results=hide>>=
library("tm")
library("XML")
@
\title{Extensions\\How to Handle Custom File Formats}
\author{Ingo Feinerer}
\maketitle

\section*{Introduction}
The possibility to handle custom file formats is a substantial feature
in any modern text mining infrastructure. \pkg{tm} has been designed
aware of this aspect from the beginning on, and has modular components
which allow extensions. A general explanation of \pkg{tm}'s extension
mechanism is described
by~\citet[Sec.~3.3]{Feinerer_etal_2008}. However, for many cases, it
is not necessary to define each detailed aspect of how to extend
\pkg{tm}. Typical examples are \acronym{XML} files which are very
common but can be rather easily handled via standard conforming
\acronym{XML} parsers. The aim of this document is to give an overview
on how simpler, more user-friendly, forms of extension mechanisms can
be applied in \pkg{tm}.

\section*{Custom General Purpose Readers}
A general situation is that you have gathered together some
information into a tabular data structure (like a data frame or a list
matrix) that suffices to describe documents in a corpus. However, you
do not have a distinct file format because you extracted the
information out of various resources. Now you want to use your
information to build a corpus which is recognized by \pkg{tm}.

This can be done in \pkg{tm} by generating a custom reader function
for tabular data structures via \code{readTabular()} that can be
configured via a so-called \emph{mapping}. A mapping describes how
your information in the tabular data structure is mapped to the
individual attributes of text documents in \pkg{tm}.

We assume that your information is put together in a data frame. E.g.,
consider the following example:
<<keep.source=TRUE>>=
df <- data.frame(contents = c("content 1", "content 2", "content 3"),
                 title    = c("title 1"  , "title 2"  , "title 3"  ),
                 authors  = c("author 1" , "author 2" , "author 3" ),
                 topics   = c("topic 1"  , "topic 2"  , "topic 3"  ),
                 stringsAsFactors = FALSE)
@
We want to map \code{contents}, \code{title}, \code{authors}, and
\code{topics} to the relevant entries of a text
document. \code{readTabular()} always returns a
\code{PlainTextDocument}, so possible attributes are:
<<>>=
names(attributes(PlainTextDocument()))
@
Except \code{class} which contains inheritance information and which
should not be modified, all other attributes can be used to access and
set predefined meta data information for a text document. Additional
user-defined meta data values can be stored via
\code{LocalMetaData}. An entry \code{Content} in the mapping will be
matched to fill the actual content of the text document.

So for our data frame we define a possible mapping as follows:
<<Mapping,keep.source=TRUE>>=
m <- list(Content = "contents", Heading = "title",
          Author = "authors", Topic = "topics")
@
Now we can construct a customized reader by passing over the
previously defined mapping:
<<myReader>>=
myReader <- readTabular(mapping = m)
@

Finally we can apply our reader function at any place where \pkg{tm}
expects a reader. E.g., we can construct a corpus out of the data
frame:
<<>>=
(corpus <- Corpus(DataframeSource(df), readerControl = list(reader = myReader)))
@

As we see the information is mapped as we want to the individual attributes
of each document:
<<>>=
corpus[[1]]
meta(corpus[[1]])
@

\section*{Custom XML Sources}
Many modern file formats already come in \acronym{XML} format which
allows to extract information with any \acronym{XML} conforming
parser, e.g., as implemented in \proglang{R} by the \pkg{XML}
package.

Now assume we have some custom \acronym{XML} format which we want to
access with \pkg{tm}. Then a viable way is to create a custom
\acronym{XML} source which can be configured with only a few
commands. E.g., have a look at the following example:
<<CustomXMLFile>>=
custom.xml <- system.file("texts", "custom.xml", package = "tm")
print(readLines(custom.xml), quote = FALSE)
@
As you see there is a top-level tag stating that there is a corpus,
and several document tags below. In fact, this structure is very
common in \acronym{XML} files found in text mining applications (e.g.,
both the Reuters-21578 and the Reuter Corpus Volume 1 data sets follow
this general scheme). In \pkg{tm} we expect a source to deliver
self-contained blocks of information to a reader function, each block
containing all information necessary such that the reader can
construct a (subclass of a) \code{TextDocument} from it.

The \code{XMLSource()} function can now be used to construct a custom
\acronym{XML} source. It has four arguments:
\begin{description}
\item[x] either a character identifying a file or a connection,
\item[parser] a function accepting an \acronym{XML} tree (as delivered by
  \code{xmlTreeParse()} in package \pkg{XML}) as input and returning a
  list of \acronym{XML} elements (each list element will then be delivered to
  the reader as such a self-contained block),
\item[reader] a reader function capable of turning \acronym{XML}
  elements as returned by the parser into a subclass of
  \code{TextDocument},
\item[encoding] a character giving the encoding of \code{x}.
\end{description}
E.g., a custom source which can cope with our custom \acronym{XML}
format could be:
<<mySource,keep.source=TRUE>>=
mySource <- function(x, encoding = "UTF-8")
    XMLSource(x, function(tree) XML::xmlRoot(tree)$children, myXMLReader, encoding)
@
As you notice in this example we also provide a custom reader function
(\code{myXMLReader}). See the next section for details.

\section*{Custom XML Readers}
As we saw in the previous section we often need a custom reader
function to extract information out of \acronym{XML} chunks (typically
as delivered by some source). Fortunately, \pkg{tm} provides an easy
way to define custom \acronym{XML} reader functions. All you need to
do is to provide a so-called \emph{specification}.

Let us start with an example which defines a reader function for the
file format from the previous section:
<<myXMLReader,keep.source=TRUE>>=
myXMLReader <- readXML(
    spec = list(Author = list("node", "/document/writer"),
                Content = list("node", "/document/description"),
                DateTimeStamp = list("function",
                                     function(x) as.POSIXlt(Sys.time(), tz = "GMT")),
                Description = list("attribute", "/document/@short"),
                Heading = list("node", "/document/caption"),
                ID = list("function", function(x) tempfile()),
                Origin = list("unevaluated", "My private bibliography"),
                Type = list("node", "/document/type")),
    doc = PlainTextDocument())
@

Formally, \code{readXML()} is the relevant function which constructs
an reader. The customization is done via the first argument
\code{spec}, the second provides an empty instance of the document
which should be returned (of course augmented with the extracted
information out of the \acronym{XML} chunks). The specification must
consist of a named list of lists each containing two character
vectors. The constructed reader will map each list entry to the
content or a meta datum of the text document as specified by the named
list entry. Valid names include \code{Content} to access the
document's content, any valid attribute name (\code{Author},
\code{DateTimeStamp}, \code{Description}, \code{Heading}, \code{ID},
and \code{Origin} in above example specification), and characters
(\code{Type} in above specification) which are mapped to so-called
\code{LocalMetaData} entries.

Each list entry must consist of two character vectors: the first
describes the type of the second argument, and the second is the
specification entry. Valid combinations are:
\begin{description}
\item[type = "node", spec = "XPathExpression"] the XPath expression
  \code{spec} extracts information out of an \acronym{XML} node (as seen for
  \code{Author}, \code{Content}, \code{Heading}, and \code{Type} in our
  example specification).
\item[type = "attribute", spec = "XPathExpression"] the XPath
  expression \code{spec} extracts information from an attribute
  of an \acronym{XML} node (like \code{Description} in our example).
\item[type = "function", spec = function(tree) \ldots] The function
  \code{spec} is called, passing over a tree representation (as
  delivered by \code{xmlInternalTreeParse()} from package \pkg{XML})
  of the read in \acronym{XML} document as first argument (as seen for
  \code{DateTimeStamp} and \code{ID}). As you notice in our example
  nobody forces us to actually use the passed over tree, instead we
  can do anything we want (e.g., create a unique character vector via
  \code{tempfile()} to have a unique identification string).
\item[type = "unevaluated", spec = "String"] the character vector
  \code{spec} is returned without modification (e.g., \code{Origin} in
  our specification).
\end{description}

Now that we have all we need to cope with our custom file format, we
can apply the source and reader function at any place in \pkg{tm}
where a source or reader is expected, respectively. E.g.,
<<>>=
corpus <- Corpus(mySource(custom.xml))
@
constructs a corpus out of the information in our \acronym{XML}
file:
<<>>=
corpus[[1]]
meta(corpus[[1]])
@

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
