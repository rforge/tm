\documentclass[a4paper]{article}

\usepackage[margin=2cm]{geometry}
\usepackage[round]{natbib}
\usepackage{url}

\newcommand{\acronym}[1]{\textsc{#1}}
\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\let\code\texttt

%% \VignetteIndexEntry{Extensions}

\begin{document}
<<Init,echo=FALSE,results=hide>>=
library("tm")
library("XML")
@
\title{Extensions\\How to Handle Custom File Formats}
\author{Ingo Feinerer}
\maketitle

\section*{Introduction}

The possibility to handle custom file formats is a substantial feature in any
modern text mining infrastructure. \pkg{tm} has been designed aware of this
aspect from the beginning on, and has modular components which allow
extensions. A general explanation of \pkg{tm}'s extension mechanism is
described by~\citet[Sec.~3.3]{Feinerer_etal_2008}, with an updated description
as follows.

\section*{Sources}
A source abstracts input locations and provides uniform methods for access.
Internally a source is represented as a \code{list} with the class attribute
\code{Source} and has the following components:
\begin{description}
\item[DefaultReader] a reader function suitable for processing the elements
  delivered by the source,
\item[Encoding] a character specifying the encoding of the elements
  delivered by the source,
\item[Length] a numeric denoting the number of elements delivered by the source
  (set to \code{NA} if unknown),
\item[Names] a character vector giving element names,
\item[Position] a numeric indicating the position in the source, and
\item[Vectorized] a logical indicating the ability for parallel element access.
\end{description}
Custom sources are required to inherit from this virtual base class
\code{Source} and typically extend this internal list structure with additional
named components necessary for representing and storing information for
document access. The function \code{Source()} is a constructor with the
signature
\code{Source(defaultReader, encoding, length, names, position, vectorized, class)}
and should be used when creating custom sources as it populates the
corresponding list components and ensures the correct base class.

Each source must provide implementations for following interface functions:
\begin{description}
\item[eoi()] returns \code{TRUE} if the end of input of the source is reached,
\item[getElem()] fetches the element at the current position,
\item[pGetElem()] if \code{Vectorized} is set, retrieves all elements in
  parallel at once, and
\item[stepNext()] increases the position in the source to the next element.
\end{description}
Retrieved elements must be encapsulated in a list with the named components
\code{content} holding the document and \code{uri} pointing to the origin of
the document (e.g., a file path or a connection; \code{NA} if not applicable or
unavailable).

E.g., a simple source which accepts an \proglang{R} vector as input could be
defined as
<<keep.source=TRUE>>=
VecSource <- function(x) {
    s <- Source(length = length(x), names = names(x), class = "VectorSource")
    s$Content <- as.character(x)
    s
}
@
which overrides a few defaults (see \code{?Source} for defaults) and stores the
vector in the \code{Content} list component. The functions \code{stepNext()} and
\code{eoi()} have reasonable default methods already for the \code{Source}
base class (basically just incrementing the \code{Position} component and
comparing the current position with the number of available elements as claimed
by \code{Length}, respectively), so we only need custom methods for element
access:
<<keep.source=TRUE>>=
getElem.VectorSource <-
function(x) list(content = x$Content[x$Position], uri = NA)
pGetElem.VectorSource <-
function(x) lapply(x$Content, function(y) list(content = y, uri = NA))
@

\section*{Readers}
Readers are functions for extracting textual content and meta data out of
elements delivered by a source and for constructing a text document. Each reader
must accept following arguments in its signature:
\begin{description}
\item[elem] a list with the named components \code{content} and \code{uri} (as
  delivered by a source via \code{getElem()} or \code{pGetElem()}),
\item[language] a string giving the text's language, and
\item[id] a unique identification string for the returned text document.
\end{description}
The element is typically provided by a source whereas the language and the
identifier are normally provided by a corpus constructor (for the case that
\code{elem\$content} does not give information on these two essential items).
In case a reader expects configuration arguments we can use a function
generator (see \code{?FunctionGenerator} for details). It allows us to process
additional arguments, store them in an environment, return a reader function
with the well-defined signature described above, and still able to access the
additional arguments via lexical scoping. The corpus constructor \code{Corpus()}
will check the function for being a function generator and if so apply it to
yield the reader with the expected signature.

E.g., the reader function \code{readPlain()} is defined as
<<keep.source=TRUE>>=
readPlain <-
function(elem, language, id)
    PlainTextDocument(elem$content, id = id, language = language)
@
For examples on readers using the function generator please have a look at
\code{?readPDF} or \code{?readTabular}.

However, for many cases, it is not necessary to define each detailed aspect of
how to extend \pkg{tm}. Typical examples are \acronym{XML} files which are very
common but can be rather easily handled via standard conforming \acronym{XML}
parsers. The aim of the remainder in this document is to give an overview on
how simpler, more user-friendly, forms of extension mechanisms can be applied
in \pkg{tm}.

\section*{Custom General Purpose Readers}
A general situation is that you have gathered together some
information into a tabular data structure (like a data frame or a list
matrix) that suffices to describe documents in a corpus. However, you
do not have a distinct file format because you extracted the
information out of various resources. Now you want to use your
information to build a corpus which is recognized by \pkg{tm}.

This can be done in \pkg{tm} by generating a custom reader function
for tabular data structures via \code{readTabular()} that can be
configured via a so-called \emph{mapping}. A mapping describes how
your information in the tabular data structure is mapped to the
individual attributes of text documents in \pkg{tm}.

We assume that your information is put together in a data frame. E.g.,
consider the following example:
<<keep.source=TRUE>>=
df <- data.frame(contents = c("content 1", "content 2", "content 3"),
                 title    = c("title 1"  , "title 2"  , "title 3"  ),
                 authors  = c("author 1" , "author 2" , "author 3" ),
                 topics   = c("topic 1"  , "topic 2"  , "topic 3"  ),
                 stringsAsFactors = FALSE)
@
We want to map \code{contents}, \code{title}, \code{authors}, and
\code{topics} to the relevant entries of a text
document. \code{readTabular()} always returns a
\code{PlainTextDocument}, so possible attributes are:
<<>>=
names(attributes(PlainTextDocument()))
@
Except \code{class} which contains inheritance information and which
should not be modified, all other attributes can be used to access and
set predefined meta data information for a text document. Additional
user-defined meta data values can be stored via
\code{LocalMetaData}. An entry \code{Content} in the mapping will be
matched to fill the actual content of the text document.

So for our data frame we define a possible mapping as follows:
<<Mapping,keep.source=TRUE>>=
m <- list(Content = "contents", Heading = "title",
          Author = "authors", Topic = "topics")
@
Now we can construct a customized reader by passing over the
previously defined mapping:
<<myReader>>=
myReader <- readTabular(mapping = m)
@

Finally we can apply our reader function at any place where \pkg{tm}
expects a reader. E.g., we can construct a corpus out of the data
frame:
<<>>=
(corpus <- Corpus(DataframeSource(df), readerControl = list(reader = myReader)))
@

As we see the information is mapped as we want to the individual attributes
of each document:
<<>>=
corpus[[1]]
meta(corpus[[1]])
@

\section*{Custom XML Sources}
Many modern file formats already come in \acronym{XML} format which
allows to extract information with any \acronym{XML} conforming
parser, e.g., as implemented in \proglang{R} by the \pkg{XML}
package.

Now assume we have some custom \acronym{XML} format which we want to
access with \pkg{tm}. Then a viable way is to create a custom
\acronym{XML} source which can be configured with only a few
commands. E.g., have a look at the following example:
<<CustomXMLFile>>=
custom.xml <- system.file("texts", "custom.xml", package = "tm")
print(readLines(custom.xml), quote = FALSE)
@
As you see there is a top-level tag stating that there is a corpus,
and several document tags below. In fact, this structure is very
common in \acronym{XML} files found in text mining applications (e.g.,
both the Reuters-21578 and the Reuters Corpus Volume 1 data sets follow
this general scheme). In \pkg{tm} we expect a source to deliver
self-contained blocks of information to a reader function, each block
containing all information necessary such that the reader can
construct a (subclass of a) \code{TextDocument} from it.

The \code{XMLSource()} function can now be used to construct a custom
\acronym{XML} source. It has four arguments:
\begin{description}
\item[x] either a character identifying a file or a connection,
\item[parser] a function accepting an \acronym{XML} tree (as delivered by
  \code{xmlTreeParse()} in package \pkg{XML}) as input and returning a
  list of \acronym{XML} elements (each list element will then be delivered to
  the reader as such a self-contained block),
\item[reader] a reader function capable of turning \acronym{XML}
  elements as returned by the parser into a subclass of
  \code{TextDocument},
\item[encoding] a character giving the encoding of \code{x}.
\end{description}
E.g., a custom source which can cope with our custom \acronym{XML}
format could be:
<<mySource,keep.source=TRUE>>=
mySource <- function(x, encoding = "UTF-8")
    XMLSource(x, function(tree) XML::xmlChildren(XML::xmlRoot(tree)), myXMLReader, encoding)
@
As you notice in this example we also provide a custom reader function
(\code{myXMLReader}). See the next section for details.

\section*{Custom XML Readers}
As we saw in the previous section we often need a custom reader
function to extract information out of \acronym{XML} chunks (typically
as delivered by some source). Fortunately, \pkg{tm} provides an easy
way to define custom \acronym{XML} reader functions. All you need to
do is to provide a so-called \emph{specification}.

Let us start with an example which defines a reader function for the
file format from the previous section:
<<myXMLReader,keep.source=TRUE>>=
myXMLReader <- readXML(
    spec = list(Author = list("node", "/document/writer"),
                Content = list("node", "/document/description"),
                DateTimeStamp = list("function",
                                     function(x) as.POSIXlt(Sys.time(), tz = "GMT")),
                Description = list("attribute", "/document/@short"),
                Heading = list("node", "/document/caption"),
                ID = list("function", function(x) tempfile()),
                Origin = list("unevaluated", "My private bibliography"),
                Type = list("node", "/document/type")),
    doc = PlainTextDocument())
@

Formally, \code{readXML()} is the relevant function which constructs
an reader. The customization is done via the first argument
\code{spec}, the second provides an empty instance of the document
which should be returned (of course augmented with the extracted
information out of the \acronym{XML} chunks). The specification must
consist of a named list of lists each containing two character
vectors. The constructed reader will map each list entry to the
content or a meta datum of the text document as specified by the named
list entry. Valid names include \code{Content} to access the
document's content, any valid attribute name (\code{Author},
\code{DateTimeStamp}, \code{Description}, \code{Heading}, \code{ID},
and \code{Origin} in above example specification), and characters
(\code{Type} in above specification) which are mapped to so-called
\code{LocalMetaData} entries.

Each list entry must consist of two character vectors: the first
describes the type of the second argument, and the second is the
specification entry. Valid combinations are:
\begin{description}
\item[\code{type = "node", spec = "XPathExpression"}] the XPath expression
  \code{spec} extracts information out of an \acronym{XML} node (as seen for
  \code{Author}, \code{Content}, \code{Heading}, and \code{Type} in our
  example specification).
\item[\code{type = "attribute", spec = "XPathExpression"}] the XPath
  expression \code{spec} extracts information from an attribute
  of an \acronym{XML} node (like \code{Description} in our example).
\item[\code{type = "function", spec = function(tree) \ldots}] The function
  \code{spec} is called, passing over a tree representation (as
  delivered by \code{xmlInternalTreeParse()} from package \pkg{XML})
  of the read in \acronym{XML} document as first argument (as seen for
  \code{DateTimeStamp} and \code{ID}). As you notice in our example
  nobody forces us to actually use the passed over tree, instead we
  can do anything we want (e.g., create a unique character vector via
  \code{tempfile()} to have a unique identification string).
\item[\code{type = "unevaluated", spec = "String"}] the character vector
  \code{spec} is returned without modification (e.g., \code{Origin} in
  our specification).
\end{description}

Now that we have all we need to cope with our custom file format, we
can apply the source and reader function at any place in \pkg{tm}
where a source or reader is expected, respectively. E.g.,
<<>>=
corpus <- Corpus(mySource(custom.xml))
@
constructs a corpus out of the information in our \acronym{XML}
file:
<<>>=
corpus[[1]]
meta(corpus[[1]])
@

\bibliographystyle{abbrvnat}
\bibliography{references}

\end{document}
