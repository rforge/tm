\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{url}
\DeclareUnicodeCharacter{201C}{"}
\DeclareUnicodeCharacter{201D}{"}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\acronym}[1]{\textsc{#1}}

%% \VignetteIndexEntry{Introduction to the tm Package}

\begin{document}
<<Init,echo=FALSE>>=
options(width = 60)
library("tm")
data("crude")
@
\title{Introduction to the \pkg{tm} Package\\Text Mining in \proglang{R}}
\author{Ingo Feinerer}
\maketitle
\sloppy

\section*{Introduction}
This vignette gives a short introduction to text mining in
\proglang{R} utilizing the text mining framework provided by the
\pkg{tm} package. We present methods for data import, corpus
handling, preprocessing, meta data management, and creation of
term-document matrices. Our focus is on the main
aspects of getting started with text mining in \proglang{R}---an in-depth
description of the text mining infrastructure offered by \pkg{tm} was
published in the \emph{Journal of Statistical Software} and is available at
\url{http://www.jstatsoft.org/v25/i05}.

\section*{Data Import}
The main structure for managing documents in \pkg{tm} is a so-called
\class{Corpus}, representing a collection of text documents. A corpus
can be created via its constructor
\code{Corpus(object, readerControl, dbControl)}.

\code{object} must be a \class{Source} object which abstracts the
input location. Available sources provided by \pkg{tm} are
\class{DirSource}, \class{VectorSource}, \class{CSVSource},
\class{GmaneSource} and \class{ReutersSource} which handle a
directory, a vector interpreting each component as document, \acronym{Csv}
files, a \acronym{Rss} feed as delivered by the Gmane mailing list
archive service, and a Reuters file containing several documents,
respectively.  Except \class{DirSource}, which is designed
solely for directories on a file system, and \class{VectorSource},
which only accepts (character) vectors, all other implemented sources
can take connections as input (a character string is interpreted as
file path). \code{getSources()} lists available sources, and the
user can create his own sources.

\code{readerControl} has to be a list with the named components
\code{reader}, \code{language}, and \code{load}. The first component
\code{reader} constructs a text document from elements delivered by a
source. The \pkg{tm} package ships with several readers (\code{readPlain()}
(default), \code{readRCV1()}, \code{readReut21578XML()},
\code{readGmane()}, \code{readNewsgroup()}, \code{readPDF()},
\code{readDOC()} and \code{readHTML()}). See \code{getReaders()} for
an up-to-date list of available readers. Each source has a default
reader which can be overridden. E.g., for \code{DirSource} the default
just reads in the input files and interprets their content as
text. The second component \code{language} sets the texts' language,
the third component \code{load} can activate lazy document loading,
i.e., whether documents should be immediately loaded into memory or
not.

Finally \code{dbControl} has to be a list with the named components
\code{useDb} indicating that database support should be activated,
\code{dbName} giving the filename holding the sourced out objects
(i.e., the database), and  \code{dbType} holding a valid database type
as supported by package \pkg{filehash}. Activated database support reduces
the memory demand, however, access gets slower since each operation
is limited by the hard disk's read and write capabilities.

So e.g., plain text files in the directory \code{txt} containing Latin
(\code{la}) texts by the Roman poet \emph{Ovid} can be read in with
following code:
<<Ovid,keep.source=TRUE>>=
txt <- system.file("texts", "txt", package = "tm")
(ovid <- Corpus(DirSource(txt),
                readerControl = list(reader = readPlain,
                                     language = "la")))
@
Another example could be mails from newsgroups (as found in the
\acronym{Uci} \acronym{Kdd} newsgroup data set):
<<Mails,keep.source=TRUE>>=
newsgroup <- system.file("texts", "newsgroup", package = "tm")

Corpus(DirSource(newsgroup),
       readerControl = list(reader = readNewsgroup,
                            language = "en_US"))
@
or \acronym{Pdf} documents:
<<PDF,keep.source=TRUE>>=
pdf <- system.file("texts", "pdf", package = "tm")

Corpus(DirSource(pdf),
       readerControl = list(reader = readPDF))
@
Note that \code{readPDF()} needs \code{pdftotext} and \code{pdfinfo}
installed on your system to be able to extract the text and meta
information from your \acronym{Pdf}s.

For simple examples \code{VectorSource} is quite useful, as it can
create a corpus from simple character vectors, e.g.:
<<VectorSource,keep.source=TRUE>>=
docs <- c("This is a text.", "This another one.")
Corpus(VectorSource(docs))
@

Finally we create a corpus for some Reuters documents as example for
later use:
<<Reuters,keep.source=TRUE>>=
reut21578 <- system.file("texts", "reut21578", package = "tm")
reuters <- Corpus(DirSource(reut21578),
                  readerControl = list(reader = readReut21578XML))
@

\section*{Data Export}
For the case you have created a text collection via manipulating other
objects in \proglang{R}, thus do not have the texts already stored on
a hard disk, and want to save the text documents to disk, you can
simply use standard \proglang{R} routines for writing out plain text
documents. E.g.,
<<eval=FALSE,keep.source=TRUE>>=
lapply(ovid,
       function(x) writeLines(x, paste(ID(x), ".txt", sep = "")))
@
Alternatively there is the function \code{writeCorpus()} which
encapsulates this functionality.

\section*{Inspecting Corpora}
Custom \code{show()} and \code{summary()} methods are available, which
hide the raw amount of information (consider a collection could
consists of several thousand documents, like a
database). \code{summary()} gives more details on meta data than
\code{show()}, whereas the full content of text documents is displayed
with \code{inspect()} on a collection.
<<>>=
inspect(ovid[1:2])
@

\section*{Transformations}
Once we have a text document collection we typically want to modify
the documents in it, e.g., stemming, stopword removal, et cetera. In
\pkg{tm}, all this functionality is subsumed into the concept of
\emph{transformation}s. Transformations are done via the \code{tmMap}
function which applies a function to all elements of the
collection. Basically, all transformations work on single text documents
and \code{tmMap} just applies them to all documents in a document
collection.

\subsection*{Converting to Plain Text Documents}
The text document collection \code{reuters} contains documents
in \acronym{Xml} format. We have no further use for the \acronym{Xml}
interna and just want to work with the text content. This can be done
by converting the documents to plain text documents. It is done by the
generic \code{asPlain()}.
<<>>=
reuters <- tmMap(reuters, asPlain)
@

\subsection*{Eliminating Extra Whitespace}
Extra whitespace is eliminated by:
<<>>=
reuters <- tmMap(reuters, stripWhitespace)
@

\subsection*{Convert to Lower Case}
Conversion to lower case by:
<<>>=
reuters <- tmMap(reuters, tmTolower)
@

\subsection*{Remove Stopwords}
Removal of stopwords by:
<<>>=
reuters <- tmMap(reuters, removeWords, stopwords("english"))
@

\subsection*{Stemming}
Stemming is done by:
<<>>=
tmMap(reuters, stemDoc)
@

\section*{Filters}
Often it is of special interest to filter out documents satisfying given
properties. For this purpose the function \code{tmFilter} is
designed. It is possible to write custom filter functions, but for
most cases the default filter does its job: it integrates a minimal
query language to filter meta data. Statements in this query language
are statements as used for subsetting data frames.

E.g., the following statement filters out those documents having the string
``\code{COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE}'' as their
heading and an \code{ID} equal to 10 (both are meta data slot
variables of the text document).
<<keep.source=TRUE>>=
query <- "identifier == '10' &
  heading == 'COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE'"
tmFilter(reuters, query)
@

There is also a full text search filter available which accepts regular
expressions:
<<keep.source=TRUE>>=
tmFilter(reuters, FUN = searchFullText,
         pattern = "partnership", doclevel = TRUE)
@

\section*{Meta Data Management}
Meta data is used to annotate text documents or whole corpora with
additional information. The easiest way to accomplish this with
\pkg{tm} is to use the \code{meta()} function. A text document has a
few predefined slots like \code{Author}, but can be extended with an
arbitrary number of local meta data tags. Alternatively to \code{meta()}
the function \code{DublinCore()} provides a full mapping
between Simple Dublin Core meta data and \pkg{tm} meta data structures
and can be similarly used to get and set meta data information for
text documents, e.g.:
<<DublinCore>>=
DublinCore(crude[[1]], "Creator") <- "Ano Nymous"
meta(crude[[1]])
@

For corpora the story is a bit more difficult. Text document
collections in \pkg{tm} have two types of meta data: one is the meta
data on the document collection level (\code{corpus} level), the other
is the meta data related to the individual documents (\code{indexed}
level) in form of a data frame. The latter is often done for
performance reasons (hence the named \code{indexed} for indexing) or
because the meta data has an own entity but still relates directly to
individual text documents, e.g., a classification result; the
classifications directly relate to the documents, but the set of classification
levels forms an own entity. Both cases can be handled with \code{meta()}:
<<>>=
meta(crude, tag = "test", type = "corpus") <- "test meta"
meta(crude, type = "corpus")
meta(crude, "foo") <- letters[1:20]
meta(crude)
@

\section*{Standard Operators and Functions}
Many standard operators and functions (\code{[}, \code{[<-}, \code{[[}, \code{[[<-},
\code{c()}, \code{length()}, \code{lapply()}, \code{sapply}()) are
available for text document collections with semantics similar to standard \proglang{R}
routines. E.g., \code{c()} concatenates two (or more) text document
collections. Applied to several text documents it returns a text
document collection. The meta data is automatically updated, if text
document collections are concatenated (i.e., merged).

There is also a custom element-of operator---it checks whether a text
document is already in a text document collection (meta data is not
checked, only the corpus):
<<>>=
reuters[[1]] %IN% reuters
crude[[1]] %IN% reuters
@

\section*{Creating Term-Document Matrices}
A common approach in text mining is to create a term-document matrix
from a corpus. In the \pkg{tm} package the class \class{TermDocMatrix}
handles sparse matrices for text document collections.
<<>>=
tdm <- TermDocMatrix(reuters)
Data(tdm)[1:5,150:155]
@

\section*{Operations on Term-Document Matrices}
Besides the fact that on the \code{Data} part of this matrix a huge
amount of \proglang{R} functions (like clustering, classifications,
etc.) is possible, this package brings some shortcuts. Imagine we
want to find those terms that occur at least five times, then we can use
the \code{findFreqTerms()} function:
<<>>=
findFreqTerms(tdm, 5)
@
Or we want to find associations (i.e., terms which correlate) with at
least $0.97$ correlation for the term \code{crop}, then we use
\code{findAssocs()} (we only display ten arbitrary associations
found):
<<>>=
findAssocs(tdm, "crop", 0.97)[31:40]
@
The function also accepts a matrix as first argument (which does not
inherit from a term-document matrix). This matrix is then interpreted
as a correlation matrix and directly used. With this approach
different correlation measures can be employed.

Term-document matrices tend to get very big already for normal sized
data sets. Therefore we provide a method to remove \emph{sparse} terms,
i.e., terms occurring only in very few documents. Normally, this
reduces the matrix dramatically without losing significant relations
inherent to the matrix:
<<>>=
removeSparseTerms(tdm, 0.4)
@
This function call removes those terms which have at least a 40
percentage of sparse (i.e., terms occurring 0 times in a document)
elements.

\section*{Dictionary}
A dictionary is a (multi-)set of strings. It is often used to represent
relevant terms in text mining. We provide a class \class{Dictionary}
implementing such a dictionary concept. It can be created via the
\code{Dictionary()} constructor, e.g.,
<<>>=
(d <- Dictionary(c("dlrs", "crude", "oil")))
@
and may be passed over to the \code{TermDocMatrix()} constructor. Then
the created matrix is tabulated against the dictionary, i.e., only
terms from the dictionary appear in the matrix. This allows to
restrict the dimension of the matrix a priori and to focus on specific
terms for distinct text mining contexts, e.g.,
<<>>=
tdmD <- TermDocMatrix(reuters, list(dictionary = d))
Data(tdmD)
@
\end{document}
