\documentclass[a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{natbib}
\author{Ingo Feinerer\\\texttt{h0125130@wu-wien.ac.at}}
\title{Literature Review}
\date{December 19, 2005}
\begin{document}
\maketitle
\bibliographystyle{plainnat}
\begin{abstract}
We present the concepts of string kernels and various text clustering
techniques. Finally the new \texttt{textmin} \textsf{R} package is
introduced and its basic functionality is presented.
\end{abstract}
\section{String kernels}
A kernel function $\phi$ calculates the inner product between mapped elements
in a feature space, hence $\phi: D \rightarrow F,~K(d_i,d_j) = \langle
\phi(d_i),\phi(d_j) \rangle$.

A special kernel for applications with any form of text strings is the
string subsequence kernel (SSK):
\begin{align*}
  K_n(s,t) &= \sum_{u \in \Sigma^n} \langle \phi_u(s),\phi_u(t)
  \rangle\\
  &= \sum_{u \in \Sigma^n} \sum_{\mathbf{i}:u=s[\mathbf{i}]}
  \lambda^{l(\mathbf{i})} \sum_{\mathbf{j}:u=t[\mathbf{j}]}
  \lambda^{l(\mathbf{j})}\\
  &= \sum_{u \in \Sigma^n} \sum_{\mathbf{i}:u=s[\mathbf{i}]}
  \sum_{\mathbf{j}:u=t[\mathbf{j}]} \lambda^{l(\mathbf{i}) +
    l(\mathbf{j})}
\end{align*}
where $K_n$ is the subsequence kernel function for strings up to the
length $n$, $s$ and $t$ denote two strings from $\Sigma^n$, the
set of all finite strings of length $n$, and $|s|$ denotes the length
of $s$. $u$ is a subsequence of $s$, if there exist indices
$\mathbf{i} = (i_1,\ldots,i_{|u|})$, with $1 \leq i_1 < \ldots < i_{|u|}
\leq |s|$, such that $u_j = s_{i_j}$, for $j = 1,\ldots,|u|$, or
$u=s[\mathbf{i}]$ for short. $\lambda$ is a decay factor $\leq 1$.

A recursive formulation of the above kernel can be used for dynamic
programming aspects:
\begin{align*}
  K_0^{\prime}(s,t) &= 1 \text{, for all } s, t,\\
  K_i^{\prime}(s,t) &= 0 \text{, if } \min (|s|,|t|) < i,\\
  K_i(s,t) &= 0 \text{, if } \min (|s|,|t|) < i,\\
  K_i^{\prime}(sx,t) &= \lambda K_i^{\prime}(s,t) + \sum_{j:t_j=x}
  K_{i-1}^{\prime}(s,t[1:j-1])\lambda^{|t|-j+2}, i = 1,\ldots,n-1,\\
  K_n(sx,t) &= K_n(s,t) + \sum_{j:t_j=x}
  K_{n-1}^{\prime}(s,t[1:j-1])\lambda^2.
\end{align*}
Details can be found in~\cite{lodhi02}.
\section{Spectral clustering}
\cite{ng02} show an algorithm for spectral clustering.
\section{Kernel $k$-means Clustering}
As described in \cite{dhillon05}, a $k$-means algorithm tries to find
$k$ clusters for given input vectors $a_1,\ldots,a_n$ such that the
distance $\mathcal{D}$ between the vectors assigned to each cluster
and its centroid is minimized. More formally, the minimization of the
following objective function, represents a $k$-means approach:
\begin{align*}
  \mathcal{D}(\{\pi_c\}^k_{c=1}) &= \sum^k_{c=1} \sum_{a_i \in \pi_c}
  |a_i - m_c|^2
\end{align*}
where $\pi_c$ denotes the $c$-th cluster and $\{\pi_c\}^k_{c=1}$ a
clustering, respectively. $m_c$ is the centroid of cluster $\pi_c$ and
is formally defined as
\begin{align*}
m_c = \frac{\sum_{a_i \in \pi_c} a_i}{|\pi_c|}
.\end{align*}
More information on the classical $k$-means algorithms can be found in
\cite{macqueen67}.

The use of kernels allows us to seperate the clusters in a non linear
way, which is not possible with the standard approach presented so
far. There the Euclidean distance measure enforces the seperation with
a hyperplane. But in many cases, especially also for representations
of text documents, a more sophisticated approach is desirable. Now
consider the following function:
\begin{align*}
  \mathcal{D}(\{\pi_c\}^k_{c=1}) &= \sum^k_{c=1} \sum_{a_i \in \pi_c}
  |\phi(a_i) - m_c|^2
\end{align*}
with
\begin{align*}
m_c = \frac{\sum_{a_i \in \pi_c} \phi(a_i)}{|\pi_c|}
.\end{align*}
If we minimize this function we have a valid kernel $k$-means
formulation, as it can be shown that the distance computation can be
reformulated such that only inner products are calculated (see
\cite[page 3]{dhillon05} for details).

A further step is the introduction of weights in the objective
function.
\section{The \texttt{textmin} \textsf{R} Package}
The \texttt{textmin} package provides a framework for text mining
applications within \textsf{R}~\citep{r05}. It fully supports the new
\textsf{S4} class system and integrates seamlessly into the \textsf{R}
architecture.

The basic framework classes for handling text documents are:
\begin{description}
  \item[\texttt{textdocument}:]
    Encapsulates a text document, irrelevant from its origin, in one
    class. Several slots are available for additional meta data, like
    an unique identifaction number or a description.
  \item[\texttt{textdoccol}:]
    Represents a collection of text documents. The constructor
    provides import facilities for common data formats in textmining
    applications, like the \textsf{Reuters21578} news format or the
    \textsf{Reuters Corpus Volume 1} format.
  \item[\texttt{termdocmatrix}:]
    Stands for a term-document matrix with documents (in fact their id
    numbers) as rows and terms as columns. Such a term-document matrix
    can be easily built from a text document collection. A bunch of
    weighting schemes are available, like binary, term frequency or
    term frequency inverse document frequency. This class can be used
    as a fast representation for all kinds of bag-of-words textmining
    algorithms.
\end{description}

Further, this package ships with scripts to extract all possible
splits from the \textsf{Reuters21578}~\citep{lewis97} XML data.
\bibliography{literature}
\end{document}
