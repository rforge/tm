\documentclass[a4paper]{article}

\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}

%% \VignetteIndexEntry{Introduction to textmin}

\begin{document}
<<echo=FALSE>>=
options(width = 75)
### for sampling
set.seed <- 1234
@
\title{Introduction to \pkg{textmin} --- Text Mining in \proglang{R}}
\author{Ingo Feinerer}
\maketitle
\sloppy

\begin{abstract}
This vignette gives a short overview over available features in the
\pkg{textmin} package for text mining purposes in \proglang{R}.
\end{abstract}

\section{Working with \pkg{textmin}}

\subsection{Loading the Package}
Before actually working we need to load the package:
<<>>=
library("textmin")
@

\subsection{Data Import}
The main structure for managing documents is a socalled text document
collection (\class{TextDocCol}). Its constructor takes following arguments:
\begin{itemize}
\item \code{object}: a \class{Source} object which abstracts the input location
\item \code{parser}: a parser which constructs a text document from a single element
  delivered by a source. A parser must have the argument signature \code{(elem,
    lodsupport, load, id)}. The first argument is the element provided
  from the source, the second indicates whether the source supports
  load on demand, the third if the user wants to load the documents
  immediately into memory, and the fourth is a unique identification
  string.
\item \code{...}: formally if the passed over \code{parser} function is of
  class \code{function\_generator}, it is assumed to be a function
  generating a parser. This way custom parsers taking various
  parameters (specified in \code{...}) can be built, which in fact
  must produce a valid parser signature but can access additional
  parameters via lexical scoping (i.e., by the including
  environment).
\end{itemize}

Available sources are \class{DirSource}, \class{CSVSource} and
\class{ReutersSource} which handle a directory and a mixed CSV or
mixed Reuters file (mixed means several documents are in a single
file). Except \class{DirSource}, which is designated
solely for directories on a file system, all other implemented sources
can take connections as input (a character string is interpreted as
filename).

This package ships with several parsers (\code{plaintext\_parser}
(default), \code{rcv1\_parser}, \code{reut21578xml\_parser} and
\code{newsgroup\_parser}). The default just reads in the whole input
file and interprets the content as text.

Plain text files in a directory:
<<>>=
# Plain text
txt <- system.file("texts/txt/", package = "textmin")
TextDocCol(DirSource(txt, load = TRUE))
@

A single comma separated values file:
<<>>=
# Comma separated values
cars.csv <- system.file("texts/cars.csv", package = "textmin")
TextDocCol(CSVSource(cars.csv))
@

Reuters21578 files either in directory (one document per file) or a single
file (several documents per file). Note that connections can be used
as input:
<<>>=
# Reuters21578 XML
reut21578 <- system.file("texts/reut21578/", package = "textmin")
reut21578.xml <- system.file("texts/reut21578.xml", package = "textmin")
reut21578.xml.gz <- system.file("texts/reut21578.xml.gz", package = "textmin")

(reut21578.tdc <- TextDocCol(DirSource(reut21578), reut21578xml_parser))

TextDocCol(ReutersSource(reut21578.xml), reut21578xml_parser)
TextDocCol(ReutersSource(gzfile(reut21578.xml.gz)), reut21578xml_parser)
@

Analogously for files in the Reuters Corpus Volume 1 format:
<<>>=
# Reuters Corpus Volume 1
rcv1 <- system.file("texts/rcv1/", package = "textmin")
rcv1.xml <- system.file("texts/rcv1.xml", package = "textmin")

TextDocCol(DirSource(rcv1, load = TRUE), rcv1_parser)
TextDocCol(ReutersSource(rcv1.xml), rcv1_parser)
@

Or mails from newsgroups (as found in the UCI KDD newsgroup dataset):
<<>>=
# UCI KDD Newsgroup Mails
newsgroup <- system.file("texts/newsgroup/", package = "textmin")

TextDocCol(DirSource(newsgroup, load = TRUE), newsgroup_parser)
@

\subsection{Inspecting the Text Document Collection}
Custom \code{show} and \code{summary} methods are available, which
hide the raw amount of information (consider a collection could
consists of several thousand documents, like a database). In order to
actually see the content use the command \code{inspect} on a
collection.

\subsection{Transformations}
Once we have a text document collection one typically wants to modify
the documents in it, e.g., stemming, stopword removal, et cetera. All
this functionality is subsumed by the concept of
\emph{transformation}s in \pkg{textmin}. Transformations are done via
the \code{tm\_transform} function.

\subsubsection{Loading Documents into Memory}
If the source objects supports load on demand, but the user has not
enforced the package to load the input content directly into memory,
this can be done manually via \code{load\_doc}. Normally it is not
necessary to call this explicitly, as other functions working on text
corpora trigger this function for not-loaded documents.
<<>>=
reut21578.tdc <- tm_transform(reut21578.tdc, load_doc)
@

\subsubsection{Converting to Plaintext Documents}
The text document collection \code{reut21578.tdc} contains documents
in XML format. We have no further use for the XML interna and just
want to work with the text content. This can be done by converting the
documents to plaintext documents. It is done by the generic
\code{as.plaintext\_doc} in assistance by a converter function
\code{reut21578xml\_to\_plain} which knows how to actually do it.
<<>>=
reut21578.tdc <- tm_transform(reut21578.tdc, as.plaintext_doc, reut21578xml_to_plain)
@

\subsubsection{Eliminating Extra Whitespace}
Extra whitespace is eliminated by:
<<>>=
reut21578.tdc <- tm_transform(reut21578.tdc, strip_whitespace)
@

\subsubsection{Convert to Lower Case}
Conversion to lower case by:
<<>>=
reut21578.tdc <- tm_transform(reut21578.tdc, tm_tolower)
@

\subsubsection{Remove Stopwords}
Removal of stopwords by:
<<>>=
data(stopwords_en)
reut21578.tdc <- tm_transform(reut21578.tdc, remove_words, stopwords_en)
@

\subsubsection{Stemming}
Stemming is done by:
<<>>=
tm_transform(reut21578.tdc, stem_doc)
@

\subsection{Filters}
Often it is of special interest to filter out documents satisfying given
properties. For this purpose the function \code{tm\_filter} is
designated. It is possible to write custom filter functions, but for
most cases the default filter does it job: it integrates a minimal
query language to filter metadata. Statements in this query language
are logically and-conjuncted \code{tag="regexpr"} pairs in form of a
named list.

E.g., following statement filters out those documents, where
\code{COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE} is their
heading and has an \code{ID} equal to 10.
<<>>=
tm_filter(reut21578.tdc, "identifier == '10' & heading == 'COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SALE'")
@

There is also a full text search available as filter:
<<>>=
tm_filter(reut21578.tdc, FUN = fulltext_search_filter, "partnership", doclevel = TRUE)
@

\subsection{Adding Data or Metadata}
Text documents or metadata can be added to text document collections
with \code{append\_doc} and \code{append\_meta}, respectively.
<<>>=
data(crude)
reut21578.tdc <- append_doc(reut21578.tdc, crude[[1]], list())
reut21578.tdc <- append_meta(reut21578.tdc, dcmeta = list(test = c(1,2,3)), dmeta = list(cl1 = 1:11))
summary(reut21578.tdc)
DCMetaData(reut21578.tdc)
DMetaData(reut21578.tdc)
@

\subsection{Modifying or Removing Metadata}
The metadata of text document collections can be easily modified or
removed:
<<>>=
print("ToDo")
#data(crude)
#reut21578.tdc <- attach_metadata(reut21578.tdc, "test", c(1,2,3))
#DMetaData(reut21578.tdc)
#reut21578.tdc <- modify_metadata(reut21578.tdc, "test", c(4,5,6))
#DMetaData(reut21578.tdc)
#reut21578.tdc <- remove_metadata(reut21578.tdc, "test")
#DMetaData(reut21578.tdc)
@

\subsection{Tagging Metadata as Subscriptable}
Often it makes sense that a subset operation applied to a text
document collection also modifies its metadata. For this purpose
metadata elements can be tagged as subscriptable.
<<>>=
print("ToDo")
#reut21578.tdc <- attach_metadata(reut21578.tdc, "myclusters", c(1,2,1,1,2,1,1,1,1,1,2))
#reut21578.tdc <- set_subscriptable(reut21578.tdc, "myclusters")
#summary(reut21578.tdc[1:2])
#DMetaData(reut21578.tdc[1:2])
@

\subsection{Operators}
Most standard operators have been overloaded for text document
collections with semantics similar to standard \proglang{R}
routines.
E.g. \code{c} concatenates two (or more) text document
collections. Applied to several text documents it returns a text
document collection.

Note also the custom element-of operator---it checks whether a text
document is already in a text document collection (metadata is not
checked, only the corpus):
<<>>=
crude[[1]] %IN% reut21578.tdc
crude[[2]] %IN% reut21578.tdc
@

\subsection{Keeping Track of Text Document Collections}
There is a mechanism available for managing text document
collections. It is called \class{TextRepository}. A typical use would
be to save different states of a text document collection.
<<>>=
data(acq)
repo <- TextRepository(reut21578.tdc)
repo <- attach_data(repo, acq)
repo <- attach_metadata(repo, "modified", date())
summary(repo)
RepresentationMetaData(repo)
summary(repo[[1]])
summary(repo[[2]])
@

\subsection{Creating Term-Document Matrices}
A common approach in text mining is to create a term-document matrix
for given texts. In this package the class \class{TermDocMatrix}
handles this for text document collections.
<<>>=
tdm <- TermDocMatrix(reut21578.tdc)
tdm[1:8,50:55]
@

\subsection{Operations on Term-Document Matrices}
Besides the fact that on this matrix a huge amount of \proglang{R}
functions (like clustering, classifications, etc.) is possible, this
package brings some shortcuts for text mining specific. Consider we
want to find those terms that occur at least 5 times:
<<>>=
find_hf_terms(tdm, 5)
@
Or we want to find associations (i.e., terms which correlate) with at
least $0.97$ correlation for the term \code{crop}:
<<>>=
find_assocs(tdm, "crop", 0.97)
@
\end{document}
